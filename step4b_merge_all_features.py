"""
=================================================================
STEP 4B: FUSION DES 3 TYPES DE FEATURES
=================================================================

Fusionne les 3 types de features pour crÃ©er le dataset final:
  1. Features Excel (68) - dÃ©mographiques, qualitÃ©, temporelles
  2. Features Deep (64) - CNN + Transformer latent representations
  3. Features NeuroKit2 (25) - HR, HRV, intervals, entropy

TOTAL: 157 FEATURES

OUTPUT:
  â€¢ all_features/W_all_train.npz (17,182 Ã— 157)
  â€¢ all_features/W_all_val.npz (2,137 Ã— 157)
  â€¢ all_features/W_all_test.npz (2,162 Ã— 157)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pickle

print("=" * 100)
print("STEP 4B: FUSION DES 3 TYPES DE FEATURES (157 TOTAL)")
print("=" * 100)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXCEL_FILE = 'PTB_XL_ML_Features_WITH_FILENAMES.xlsx'
DEEP_DIR = Path('deep_features')
NEUROKIT_FILE = 'ptbxl_wide_features.csv'
OUTPUT_DIR = Path('all_features')
OUTPUT_DIR.mkdir(exist_ok=True)

# Colonnes Ã  exclure d'Excel (dÃ©jÃ  dans labels ou non-features)
EXCLUDE_EXCEL_COLS = [
    'ecg_id', 'patient_id', 'filename_lr', 'filename_hr',
    'report', 'validated_by', 'nurse', 'site', 'device',
    'recording_date', 'strat_fold',
    # Exclure les labels SCP (dÃ©jÃ  dans labels)
] + [f'scp_{x}' for x in ['SR', 'NORM', 'ABQRS', 'IMI', 'ASMI', 'LVH', 'NDT', 'LAFB', 
                           'AFIB', 'ISC_', 'PVC', 'IRBBB', 'STD_', 'VCLVH', 'STACH', 
                           'IVCD', '1AVB', 'SARRH', 'NST_', 'ISCAL', 'SBRAD', 'CRBBB', 
                           'QWAVE', 'CLBBB', 'ILMI', 'LOWT', 'LAO/LAE', 'NT_', 'PAC', 'AMI']] \
  + [f'scp_superclass_{x}' for x in ['NORM', 'MI', 'STTC', 'CD', 'HYP']]

print(f"\nâš™ï¸  CONFIGURATION:")
print(f"  â€¢ Output: {OUTPUT_DIR}/")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. CHARGEMENT DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n[1/5] Chargement des 3 types de features...")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. IDENTIFIER SIGNAUX NETTOYÃ‰S DISPONIBLES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  Identification des signaux nettoyÃ©s...")
SIGNALS_DIR = Path('cleaned_signals_100hz')
available_signals = []
for npz_file in SIGNALS_DIR.glob('X_clean_*.npz'):
    ecg_id = int(npz_file.stem.replace('X_clean_', ''))
    available_signals.append(ecg_id)

available_signals = set(available_signals)
print(f"    âœ“ {len(available_signals)} signaux nettoyÃ©s disponibles")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# A. Features Excel (Type 1)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  Type 1: Features Excel...")
df_train_excel = pd.read_excel(EXCEL_FILE, sheet_name='Train')
df_val_excel = pd.read_excel(EXCEL_FILE, sheet_name='Val')
df_test_excel = pd.read_excel(EXCEL_FILE, sheet_name='Test')

# FILTRER UNIQUEMENT LES ECG AVEC SIGNAUX NETTOYÃ‰S
df_train_excel = df_train_excel[df_train_excel['ecg_id'].isin(available_signals)]
df_val_excel = df_val_excel[df_val_excel['ecg_id'].isin(available_signals)]
df_test_excel = df_test_excel[df_test_excel['ecg_id'].isin(available_signals)]

print(f"    âœ“ Train: {len(df_train_excel)} ECG (filtered)")
print(f"    âœ“ Val  : {len(df_val_excel)} ECG (filtered)")
print(f"    âœ“ Test : {len(df_test_excel)} ECG (filtered)")

# SÃ©lectionner colonnes features
excel_cols = [col for col in df_train_excel.columns if col not in EXCLUDE_EXCEL_COLS]
print(f"    âœ“ {len(excel_cols)} colonnes Excel")
print(f"    Exemples: {excel_cols[:5]}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# B. Features Deep (Type 2)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  Type 2: Features Deep (CNN + Transformer)...")
deep_train = np.load(DEEP_DIR / 'deep_features_train.npz')
deep_val = np.load(DEEP_DIR / 'deep_features_val.npz')
deep_test = np.load(DEEP_DIR / 'deep_features_test.npz')

Deep_train = deep_train['features']
Deep_val = deep_val['features']
Deep_test = deep_test['features']

deep_ids_train = deep_train['ecg_ids']
deep_ids_val = deep_val['ecg_ids']
deep_ids_test = deep_test['ecg_ids']

# FILTRER LES DEEP FEATURES POUR UTILISER UNIQUEMENT LES ECG AVEC SIGNAUX
train_mask = np.isin(deep_ids_train, list(available_signals))
val_mask = np.isin(deep_ids_val, list(available_signals))
test_mask = np.isin(deep_ids_test, list(available_signals))

Deep_train = Deep_train[train_mask]
Deep_val = Deep_val[val_mask]
Deep_test = Deep_test[test_mask]

deep_ids_train = deep_ids_train[train_mask]
deep_ids_val = deep_ids_val[val_mask]
deep_ids_test = deep_ids_test[test_mask]

print(f"    âœ“ Train: {len(deep_ids_train)} ECG Ã— {Deep_train.shape[1]} features")
print(f"    âœ“ Val  : {len(deep_ids_val)} ECG Ã— {Deep_val.shape[1]} features")
print(f"    âœ“ Test : {len(deep_ids_test)} ECG Ã— {Deep_test.shape[1]} features")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# C. Features NeuroKit2 (Type 3)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  Type 3: Features NeuroKit2...")
df_nk2 = pd.read_csv(NEUROKIT_FILE, index_col='ecg_id')

# Colonnes NeuroKit2 (exclure strat_fold)
nk2_cols = [col for col in df_nk2.columns if col != 'strat_fold']
print(f"    âœ“ {len(nk2_cols)} features NeuroKit2")
print(f"    Exemples: {nk2_cols[:5]}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. FUSION DES FEATURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n[2/5] Fusion des 3 types...")

def merge_all_features(df_excel, Deep_feats, deep_ids, split_name):
    """Fusionne Excel + Deep + NeuroKit2 pour un split"""
    
    # Convertir ecg_id en index si pas dÃ©jÃ  fait
    if 'ecg_id' in df_excel.columns:
        df_excel = df_excel.set_index('ecg_id')
    
    # Aligner Deep features sur les ecg_ids
    df_deep = pd.DataFrame(
        Deep_feats,
        index=deep_ids,
        columns=[f'deep_{i:02d}' for i in range(Deep_feats.shape[1])]
    )
    
    # Aligner NeuroKit2 features
    df_nk2_split = df_nk2.loc[deep_ids, nk2_cols]
    
    # Excel features
    df_excel_split = df_excel.loc[deep_ids, excel_cols]
    
    # Fusionner les 3
    df_all = pd.concat([
        df_excel_split,    # Type 1: Excel (68)
        df_deep,           # Type 2: Deep (64)
        df_nk2_split       # Type 3: NeuroKit2 (25)
    ], axis=1)
    
    print(f"    {split_name}: {df_all.shape[0]} ECG Ã— {df_all.shape[1]} features")
    print(f"      â€¢ Excel: {len(excel_cols)}")
    print(f"      â€¢ Deep: {Deep_feats.shape[1]}")
    print(f"      â€¢ NeuroKit2: {len(nk2_cols)}")
    
    return df_all

# Fusion
df_all_train = merge_all_features(df_train_excel, Deep_train, deep_ids_train, "Train")
df_all_val = merge_all_features(df_val_excel, Deep_val, deep_ids_val, "Val")
df_all_test = merge_all_features(df_test_excel, Deep_test, deep_ids_test, "Test")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. PREPROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n[3/5] Preprocessing (imputation + scaling)...")

# Identifier colonnes numÃ©riques (toutes sauf catÃ©gorielles)
num_cols = df_all_train.select_dtypes(include=[np.number]).columns.tolist()
print(f"  âœ“ {len(num_cols)} colonnes numÃ©riques")

# Pipeline: Imputation + Scaling
preprocessor = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# FIT sur Train uniquement
W_all_train = preprocessor.fit_transform(df_all_train[num_cols])
W_all_val = preprocessor.transform(df_all_val[num_cols])
W_all_test = preprocessor.transform(df_all_test[num_cols])

print(f"  âœ“ Train: {W_all_train.shape}")
print(f"  âœ“ Val  : {W_all_val.shape}")
print(f"  âœ“ Test : {W_all_test.shape}")

# VÃ©rifier NaN
print(f"\n  ğŸ” VÃ‰RIFICATION NaN:")
print(f"    â€¢ Train: {np.isnan(W_all_train).sum()} NaN")
print(f"    â€¢ Val  : {np.isnan(W_all_val).sum()} NaN")
print(f"    â€¢ Test : {np.isnan(W_all_test).sum()} NaN")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. SAUVEGARDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n[4/5] Sauvegarde...")

np.savez_compressed(
    OUTPUT_DIR / 'W_all_train.npz',
    W=W_all_train,
    ecg_ids=deep_ids_train
)

np.savez_compressed(
    OUTPUT_DIR / 'W_all_val.npz',
    W=W_all_val,
    ecg_ids=deep_ids_val
)

np.savez_compressed(
    OUTPUT_DIR / 'W_all_test.npz',
    W=W_all_test,
    ecg_ids=deep_ids_test
)

# Sauvegarder preprocessor
with open(OUTPUT_DIR / 'preprocessor_all.pkl', 'wb') as f:
    pickle.dump(preprocessor, f)

print(f"  âœ“ SauvegardÃ©: {OUTPUT_DIR}/W_all_train.npz")
print(f"  âœ“ SauvegardÃ©: {OUTPUT_DIR}/W_all_val.npz")
print(f"  âœ“ SauvegardÃ©: {OUTPUT_DIR}/W_all_test.npz")
print(f"  âœ“ SauvegardÃ©: {OUTPUT_DIR}/preprocessor_all.pkl")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. RÃ‰SUMÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n[5/5] CrÃ©ation CSV rÃ©capitulatif...")

# Sauvegarder aussi en CSV pour inspection
df_all_train.to_csv(OUTPUT_DIR / 'all_features_train.csv')
print(f"  âœ“ SauvegardÃ©: {OUTPUT_DIR}/all_features_train.csv")

print("\n" + "=" * 100)
print("STATISTIQUES FINALES")
print("=" * 100)

print(f"\nğŸ“Š FEATURES TOTALES FUSIONNÃ‰ES:")
print(f"  â€¢ Type 1 (Excel)     : {len(excel_cols)} features")
print(f"  â€¢ Type 2 (Deep)      : {Deep_train.shape[1]} features")
print(f"  â€¢ Type 3 (NeuroKit2) : {len(nk2_cols)} features")
print(f"  â€¢ TOTAL              : {W_all_train.shape[1]} features")

print(f"\nğŸ“ˆ SPLITS:")
print(f"  â€¢ Train: {W_all_train.shape[0]} ECG Ã— {W_all_train.shape[1]} features")
print(f"  â€¢ Val  : {W_all_val.shape[0]} ECG Ã— {W_all_val.shape[1]} features")
print(f"  â€¢ Test : {W_all_test.shape[0]} ECG Ã— {W_all_test.shape[1]} features")

print(f"\nğŸ’¾ FICHIERS GÃ‰NÃ‰RÃ‰S:")
print(f"  â€¢ {OUTPUT_DIR}/W_all_train.npz")
print(f"  â€¢ {OUTPUT_DIR}/W_all_val.npz")
print(f"  â€¢ {OUTPUT_DIR}/W_all_test.npz")
print(f"  â€¢ {OUTPUT_DIR}/preprocessor_all.pkl")
print(f"  â€¢ {OUTPUT_DIR}/all_features_train.csv")

print(f"\nâœ… STEP 4B TERMINÃ‰")
print(f"   Prochaine Ã©tape: Modifier step6_training.py pour utiliser {OUTPUT_DIR}/")
print("=" * 100)
