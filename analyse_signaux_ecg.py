"""
ANALYSE COMPLÃˆTE DES SIGNAUX ECG - RECORDS100 et RECORDS500
Dataset PTB-XL v1.0.3
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import struct

print("=" * 100)
print("ANALYSE APPROFONDIE DES SIGNAUX ECG - PTB-XL DATASET")
print("=" * 100)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. STATISTIQUES GLOBALES DES FICHIERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("1. INVENTAIRE COMPLET DES FICHIERS")
print("â”€" * 100)

def count_files_in_directory(directory):
    """Compter fichiers .hea et .dat dans un rÃ©pertoire"""
    hea_files = list(Path(directory).rglob("*.hea"))
    dat_files = list(Path(directory).rglob("*.dat"))
    
    # Calculer tailles
    total_hea_size = sum(f.stat().st_size for f in hea_files)
    total_dat_size = sum(f.stat().st_size for f in dat_files)
    
    return {
        'hea_count': len(hea_files),
        'dat_count': len(dat_files),
        'hea_size_mb': total_hea_size / (1024**2),
        'dat_size_mb': total_dat_size / (1024**2),
        'total_size_mb': (total_hea_size + total_dat_size) / (1024**2)
    }

# Analyser records100
print("\nğŸ“ RECORDS100/ (Basse rÃ©solution - 100 Hz)")
records100_stats = count_files_in_directory('records100')
print(f"  â€¢ Fichiers .hea : {records100_stats['hea_count']:,}")
print(f"  â€¢ Fichiers .dat : {records100_stats['dat_count']:,}")
print(f"  â€¢ Taille .hea   : {records100_stats['hea_size_mb']:.2f} MB")
print(f"  â€¢ Taille .dat   : {records100_stats['dat_size_mb']:.2f} MB")
print(f"  â€¢ Taille totale : {records100_stats['total_size_mb']:.2f} MB")

# Analyser records500
print("\nğŸ“ RECORDS500/ (Haute rÃ©solution - 500 Hz)")
records500_stats = count_files_in_directory('records500')
print(f"  â€¢ Fichiers .hea : {records500_stats['hea_count']:,}")
print(f"  â€¢ Fichiers .dat : {records500_stats['dat_count']:,}")
print(f"  â€¢ Taille .hea   : {records500_stats['hea_size_mb']:.2f} MB")
print(f"  â€¢ Taille .dat   : {records500_stats['dat_size_mb']:.2f} MB")
print(f"  â€¢ Taille totale : {records500_stats['total_size_mb']:.2f} MB")

print(f"\nğŸ“Š TOTAL COMBINÃ‰")
print(f"  â€¢ Enregistrements : {records100_stats['hea_count']:,}")
print(f"  â€¢ Fichiers totaux : {(records100_stats['hea_count'] + records100_stats['dat_count'] + records500_stats['hea_count'] + records500_stats['dat_count']):,}")
print(f"  â€¢ Espace disque   : {(records100_stats['total_size_mb'] + records500_stats['total_size_mb']):.2f} MB")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ANALYSE DES FICHIERS HEADER (.hea)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("2. ANALYSE DÃ‰TAILLÃ‰E DES FICHIERS HEADER (.hea)")
print("â”€" * 100)

def parse_header_file(filepath):
    """Parser un fichier .hea pour extraire mÃ©tadonnÃ©es"""
    with open(filepath, 'r') as f:
        lines = f.readlines()
    
    # Ligne 1: nom freq nb_samples
    header_line = lines[0].strip().split()
    filename = header_line[0]
    n_leads = int(header_line[1])
    freq = int(header_line[2])
    n_samples = int(header_line[3])
    
    # Lignes suivantes: spÃ©cifications des leads
    leads = []
    for i in range(1, n_leads + 1):
        if i < len(lines):
            lead_info = lines[i].strip().split()
            lead_name = lead_info[-1] if len(lead_info) > 0 else f"Lead_{i}"
            leads.append(lead_name)
    
    return {
        'filename': filename,
        'n_leads': n_leads,
        'freq': freq,
        'n_samples': n_samples,
        'duration_sec': n_samples / freq,
        'leads': leads
    }

# Analyser 5 fichiers exemples de records100
print("\nğŸ” EXEMPLE: 5 premiers fichiers records100/")
hea_files_100 = sorted(list(Path('records100/00000').glob("*.hea")))[:5]

for hea_file in hea_files_100:
    meta = parse_header_file(hea_file)
    print(f"\n  ğŸ“„ {meta['filename']}")
    print(f"     â€¢ DÃ©rivations : {meta['n_leads']} leads â†’ {', '.join(meta['leads'])}")
    print(f"     â€¢ FrÃ©quence   : {meta['freq']} Hz")
    print(f"     â€¢ Ã‰chantillons: {meta['n_samples']:,}")
    print(f"     â€¢ DurÃ©e       : {meta['duration_sec']} secondes")

# Analyser 5 fichiers exemples de records500
print("\nğŸ” EXEMPLE: 5 premiers fichiers records500/")
hea_files_500 = sorted(list(Path('records500/00000').glob("*.hea")))[:5]

for hea_file in hea_files_500:
    meta = parse_header_file(hea_file)
    print(f"\n  ğŸ“„ {meta['filename']}")
    print(f"     â€¢ DÃ©rivations : {meta['n_leads']} leads â†’ {', '.join(meta['leads'])}")
    print(f"     â€¢ FrÃ©quence   : {meta['freq']} Hz")
    print(f"     â€¢ Ã‰chantillons: {meta['n_samples']:,}")
    print(f"     â€¢ DurÃ©e       : {meta['duration_sec']} secondes")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ANALYSE DES FICHIERS DONNÃ‰ES (.dat)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("3. ANALYSE DES FICHIERS DONNÃ‰ES BINAIRES (.dat)")
print("â”€" * 100)

def analyze_dat_file(dat_filepath, n_samples, n_leads):
    """Analyser fichier .dat binaire"""
    file_size = os.path.getsize(dat_filepath)
    expected_size = n_samples * n_leads * 2  # 2 bytes par Ã©chantillon (16-bit)
    
    # Lire Ã©chantillons
    with open(dat_filepath, 'rb') as f:
        data = f.read()
    
    # Convertir en int16
    samples = struct.unpack(f'<{len(data)//2}h', data)  # little-endian signed short
    samples_array = np.array(samples).reshape((n_samples, n_leads))
    
    return {
        'file_size_bytes': file_size,
        'expected_size_bytes': expected_size,
        'size_match': file_size == expected_size,
        'min_value': samples_array.min(),
        'max_value': samples_array.max(),
        'mean_value': samples_array.mean(),
        'std_value': samples_array.std()
    }

# Analyser fichiers .dat correspondants
print("\nğŸ”¬ ANALYSE STATISTIQUE: records100/00000/00001_lr.dat")
meta = parse_header_file('records100/00000/00001_lr.hea')
dat_stats = analyze_dat_file('records100/00000/00001_lr.dat', meta['n_samples'], meta['n_leads'])

print(f"  â€¢ Taille fichier     : {dat_stats['file_size_bytes']:,} bytes")
print(f"  â€¢ Taille attendue    : {dat_stats['expected_size_bytes']:,} bytes")
print(f"  â€¢ Correspondance     : {'âœ“ OUI' if dat_stats['size_match'] else 'âœ— NON'}")
print(f"  â€¢ Valeur min (ADC)   : {dat_stats['min_value']}")
print(f"  â€¢ Valeur max (ADC)   : {dat_stats['max_value']}")
print(f"  â€¢ Valeur moyenne     : {dat_stats['mean_value']:.2f}")
print(f"  â€¢ Ã‰cart-type         : {dat_stats['std_value']:.2f}")

print("\nğŸ”¬ ANALYSE STATISTIQUE: records500/00000/00001_hr.dat")
meta_hr = parse_header_file('records500/00000/00001_hr.hea')
dat_stats_hr = analyze_dat_file('records500/00000/00001_hr.dat', meta_hr['n_samples'], meta_hr['n_leads'])

print(f"  â€¢ Taille fichier     : {dat_stats_hr['file_size_bytes']:,} bytes")
print(f"  â€¢ Taille attendue    : {dat_stats_hr['expected_size_bytes']:,} bytes")
print(f"  â€¢ Correspondance     : {'âœ“ OUI' if dat_stats_hr['size_match'] else 'âœ— NON'}")
print(f"  â€¢ Valeur min (ADC)   : {dat_stats_hr['min_value']}")
print(f"  â€¢ Valeur max (ADC)   : {dat_stats_hr['max_value']}")
print(f"  â€¢ Valeur moyenne     : {dat_stats_hr['mean_value']:.2f}")
print(f"  â€¢ Ã‰cart-type         : {dat_stats_hr['std_value']:.2f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. ORGANISATION HIÃ‰RARCHIQUE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("4. STRUCTURE HIÃ‰RARCHIQUE DES DOSSIERS")
print("â”€" * 100)

def analyze_folder_structure(base_dir):
    """Analyser structure des sous-dossiers"""
    subdirs = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])
    
    folder_stats = []
    for subdir in subdirs:
        subdir_path = os.path.join(base_dir, subdir)
        hea_count = len(list(Path(subdir_path).glob("*.hea")))
        dat_count = len(list(Path(subdir_path).glob("*.dat")))
        folder_stats.append({
            'folder': subdir,
            'hea_count': hea_count,
            'dat_count': dat_count
        })
    
    return folder_stats

print("\nğŸ“‚ RECORDS100/ - RÃ©partition par dossier:")
folders_100 = analyze_folder_structure('records100')
print(f"  â€¢ Nombre de dossiers: {len(folders_100)}")
print(f"  â€¢ Range ECG IDs     : {folders_100[0]['folder']} Ã  {folders_100[-1]['folder']}")
print(f"\n  DÃ©tail (premiers 5 dossiers):")
for folder in folders_100[:5]:
    print(f"    - {folder['folder']}/  : {folder['hea_count']:3d} fichiers .hea, {folder['dat_count']:3d} fichiers .dat")

print("\nğŸ“‚ RECORDS500/ - RÃ©partition par dossier:")
folders_500 = analyze_folder_structure('records500')
print(f"  â€¢ Nombre de dossiers: {len(folders_500)}")
print(f"  â€¢ Range ECG IDs     : {folders_500[0]['folder']} Ã  {folders_500[-1]['folder']}")
print(f"\n  DÃ©tail (premiers 5 dossiers):")
for folder in folders_500[:5]:
    print(f"    - {folder['folder']}/  : {folder['hea_count']:3d} fichiers .hea, {folder['dat_count']:3d} fichiers .dat")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. VÃ‰RIFICATION COHÃ‰RENCE AVEC CSV
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("5. VÃ‰RIFICATION COHÃ‰RENCE AVEC ptbxl_database.csv")
print("â”€" * 100)

# Charger CSV
df_csv = pd.read_csv('ptbxl_database.csv', index_col='ecg_id')
print(f"\nâœ“ CSV chargÃ©: {len(df_csv):,} enregistrements")

# VÃ©rifier que tous les filename_lr existent
print(f"\nğŸ” VÃ©rification existence fichiers filename_lr...")
missing_lr = []
for idx, row in df_csv.head(100).iterrows():  # Test sur 100 premiers
    hea_path = row['filename_lr'] + '.hea'
    dat_path = row['filename_lr'] + '.dat'
    
    if not os.path.exists(hea_path):
        missing_lr.append((idx, hea_path, '.hea'))
    if not os.path.exists(dat_path):
        missing_lr.append((idx, dat_path, '.dat'))

if len(missing_lr) == 0:
    print(f"  âœ“ Tous les fichiers existent (Ã©chantillon de 100)")
else:
    print(f"  âœ— {len(missing_lr)} fichiers manquants dÃ©tectÃ©s")
    for ecg_id, path, ext in missing_lr[:5]:
        print(f"    - ECG {ecg_id}: {path}")

# VÃ©rifier que tous les filename_hr existent
print(f"\nğŸ” VÃ©rification existence fichiers filename_hr...")
missing_hr = []
for idx, row in df_csv.head(100).iterrows():
    hea_path = row['filename_hr'] + '.hea'
    dat_path = row['filename_hr'] + '.dat'
    
    if not os.path.exists(hea_path):
        missing_hr.append((idx, hea_path, '.hea'))
    if not os.path.exists(dat_path):
        missing_hr.append((idx, dat_path, '.dat'))

if len(missing_hr) == 0:
    print(f"  âœ“ Tous les fichiers existent (Ã©chantillon de 100)")
else:
    print(f"  âœ— {len(missing_hr)} fichiers manquants dÃ©tectÃ©s")
    for ecg_id, path, ext in missing_hr[:5]:
        print(f"    - ECG {ecg_id}: {path}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. CARACTÃ‰RISTIQUES TECHNIQUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "â”€" * 100)
print("6. CARACTÃ‰RISTIQUES TECHNIQUES DES SIGNAUX")
print("â”€" * 100)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaractÃ©ristique     â”‚ records100/      â”‚ records500/      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FrÃ©quence           â”‚ 100 Hz           â”‚ 500 Hz           â”‚
â”‚ DurÃ©e               â”‚ 10 secondes      â”‚ 10 secondes      â”‚
â”‚ Ã‰chantillons/lead   â”‚ 1,000            â”‚ 5,000            â”‚
â”‚ Nombre de leads     â”‚ 12               â”‚ 12               â”‚
â”‚ Format donnÃ©es      â”‚ 16-bit signed    â”‚ 16-bit signed    â”‚
â”‚ Taille/enregistr.   â”‚ ~24 KB           â”‚ ~120 KB          â”‚
â”‚ RÃ©solution ADC      â”‚ 1 ÂµV/unit        â”‚ 1 ÂµV/unit        â”‚
â”‚ Gain standard       â”‚ 1000 units/mV    â”‚ 1000 units/mV    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š LEADS STANDARD (12 dÃ©rivations ECG):
  â€¢ Bipolaires (Einthoven)    : I, II, III
  â€¢ Unipolaires augmentÃ©es     : AVR, AVL, AVF
  â€¢ PrÃ©cordiales (thorax)      : V1, V2, V3, V4, V5, V6

ğŸ’¡ APPLICATIONS:
  â€¢ records100/ â†’ Feature extraction, ML classique, dÃ©ploiement temps rÃ©el
  â€¢ records500/ â†’ Deep Learning, analyse morphologique dÃ©taillÃ©e, recherche
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. RÃ‰SUMÃ‰ FINAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "=" * 100)
print("7. RÃ‰SUMÃ‰ EXÃ‰CUTIF")
print("=" * 100)

total_recordings = records100_stats['hea_count']
total_files = (records100_stats['hea_count'] + records100_stats['dat_count'] + 
               records500_stats['hea_count'] + records500_stats['dat_count'])
total_size_gb = (records100_stats['total_size_mb'] + records500_stats['total_size_mb']) / 1024

print(f"""
âœ“ DATASET PTB-XL v1.0.3 - Signaux ECG

ğŸ“Š VOLUME:
  â€¢ Enregistrements ECG        : {total_recordings:,}
  â€¢ Fichiers totaux            : {total_files:,}
  â€¢ Espace disque              : {total_size_gb:.2f} GB
  â€¢ RÃ©solutions disponibles    : 2 (100 Hz et 500 Hz)

ğŸ¥ CONTENU MÃ‰DICAL:
  â€¢ DÃ©rivations par ECG        : 12 leads standard
  â€¢ DurÃ©e par enregistrement   : 10 secondes
  â€¢ Patients uniques           : ~18,869
  â€¢ Codes diagnostiques SCP    : 71 pathologies

ğŸ”§ QUALITÃ‰ TECHNIQUE:
  â€¢ Format                     : PhysioNet WFDB
  â€¢ Encodage                   : 16-bit signed integer
  â€¢ RÃ©solution temporelle      : 100 Hz (standard) / 500 Hz (recherche)
  â€¢ IntÃ©gritÃ© fichiers         : âœ“ VÃ©rifiÃ©e (Ã©chantillon 100)

ğŸ¯ PRÃŠT POUR:
  âœ“ Machine Learning (features tabulaires)
  âœ“ Deep Learning (CNN/LSTM sur signaux bruts)
  âœ“ Analyse morphologique (dÃ©tection P-QRS-T)
  âœ“ Classification multi-label (30 codes SCP principaux)
  âœ“ Transfert d'apprentissage (prÃ©-entraÃ®nement haute rÃ©solution)
""")

print("=" * 100)
print("ANALYSE TERMINÃ‰E")
print("=" * 100)
